{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Author Attribution\n",
    "\n",
    "## Identifying authors through excerpts\n",
    "\n",
    "------\n",
    "\n",
    "A notebook using Python alongside sklearn and NLTK to develop an NLP classifier. Given a collection of data on six Portuguese authors, we first analyze the data and take some general decisions on how to tackle the task. We make use of NLTK features to clean the data alongside some BASH scripts. The models are built and trained with sklearn.\n",
    "\n",
    "Final predictions based on best model:\n",
    "\n",
    "1000 word excerpts:\n",
    "\n",
    "- testData/1000words/text1_clean.txt: joseSaramago\n",
    "- testData/1000words/text2_clean.txt: almadaNegreiros\n",
    "- testData/1000words/text3_clean.txt: luisaMarquesSilva\n",
    "- testData/1000words/text4_clean.txt: ecaDeQueiros\n",
    "- testData/1000words/text5_clean.txt: camiloCasteloBranco\n",
    "- testData/1000words/text6_clean.txt: joseRodriguesSantos\n",
    "\n",
    "500 word excerpts:\n",
    "\n",
    "- testData/500words/text1_clean.txt: joseSaramago\n",
    "- testData/500words/text2_clean.txt: almadaNegreiros\n",
    "- testData/500words/text3_clean.txt: luisaMarquesSilva\n",
    "- testData/500words/text4_clean.txt: ecaDeQueiros\n",
    "- testData/500words/text5_clean.txt: camiloCasteloBranco\n",
    "- testData/5000words/text6_clean.txt: joseRodriguesSantos\n",
    "\n",
    "------\n",
    "Authors: \n",
    "- Davide Montali M20190201\n",
    "- Francisco Cruz M20190637\n",
    "- Umberto Tammaro M20190806\n",
    "\n",
    "Course: Text Mining -- Nova IMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "# Requirements and Imports\n",
    "\n",
    "Please ensure you have the dependencies below installed and make sure to download the required NLTK data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from itertools import count\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/d4ve/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please make sure you have the following parts downloaded\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalising\n",
    "\n",
    "We use a quick bash script to normalize the data. We first of all translate the data into ascii standard, to remove the various portuguese accents. Then we take the complement of the following set: 'A-Za-z0-9-,.!?\"' - we decided to keep the punctuation given the varying use of punctuation by the different authors. We also lowercase the documents and merge them into a corpus document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf trainData/*/*clean.txt\n",
    "rm -rf trainData/*/corpus.txt\n",
    "for dir in trainData/*/; do\n",
    "    for book in $dir*.txt; do\n",
    "        iconv -f utf8 -t ascii//TRANSLIT \"$book\" | tr -sc 'A-Za-z0-9-,.!?\"' ' ' | tr A-Z a-z  > \"${book%.*}\"_clean.txt\n",
    "        cat \"${book%.*}\"_clean.txt >> \"${dir}corpus.txt\"\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also apply the same script to the data to be identified, although here for obvious reasons we don't concatenate the files into a corpus document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf testData/*/*clean.txt\n",
    "for dir in testData/*/; do\n",
    "    for book in $dir*.txt; do\n",
    "        iconv -f utf8 -t ascii//TRANSLIT \"$book\" | tr -sc 'A-Za-z0-9-,.!?\"' ' ' | tr A-Z a-z  > \"${book%.*}\"_clean.txt\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our functions used throughout the notebook as well as some variables we use. We also load the NLTK portuguese stemmer and stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_author(path):\n",
    "    \"\"\"Determine the author of a book through its file path.\"\"\"\n",
    "    for key, author in authors.items():\n",
    "        if author in path:\n",
    "            return author\n",
    "        \n",
    "def clean_doc(doc, stopwords=True):\n",
    "    doc = stem_doc(doc)\n",
    "    if stopwords == True:\n",
    "        doc = stop_doc(doc)\n",
    "    return doc\n",
    "\n",
    "def stem_doc(doc):\n",
    "    \"\"\"Takes a document, splits it up and stemms each word - then\n",
    "    remerges the document together and returns it.\"\"\"\n",
    "    doc_split = doc.split()\n",
    "    stem = [stemmer.stem(str(i)) for i in doc_split]\n",
    "    doc = ' '.join(stem)\n",
    "    return doc\n",
    "\n",
    "def stop_doc(doc):\n",
    "    \"\"\"Takes a document and removes all stopwords from it\"\"\"\n",
    "    doc_split = doc.split()\n",
    "    temp = [i for i in doc_split if i not in stopwords]\n",
    "    doc = ' '.join(temp)\n",
    "    return doc\n",
    "\n",
    "def clean_new_data(doc):\n",
    "    doc = re.split(r'(\\W+)', doc)\n",
    "    doc = ' '.join(doc)\n",
    "    doc = clean_doc(doc)\n",
    "    return doc\n",
    "\n",
    "authors = {\n",
    "            1: \"almadaNegreiros\",\n",
    "            2: \"ecaDeQueiros\",\n",
    "            3: \"joseSaramago\",\n",
    "            4: \"camiloCasteloBranco\",\n",
    "            5: \"joseRodriguesSantos\",\n",
    "            6: \"luisaMarquesSilva\"}\n",
    "\n",
    "# NLTK Tools\n",
    "stopwords = list(nltk.corpus.stopwords.words('portuguese'))\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "# Training data paths\n",
    "paths = glob.glob('trainData/*/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing\n",
    "\n",
    "Given the large imbalance in terms of words per author we have, we decided to split the corpus of each author into a number of smaller documents. Below we split into 500 word documents - the same count as the shortest excerpts we are looking to predict.\n",
    "\n",
    "We also decided to undersample our data - this is further motivated by the vast imbalance ratio between the different auhtors. We explored oversampling as well - but given that we would generate new data for our minority class on the already scarce data we have, we decided undersampling was a more sensible route.\n",
    "\n",
    "We store our data in a separate folder, with documents within a subfolder named after the authors (our target variable) - using this format we can easily read our data into sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [00:00<00:00, 18994.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting {map_author(path)}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 110/110 [00:00<00:00, 20200.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting {map_author(path)}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 301/301 [00:00<00:00, 21189.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting {map_author(path)}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 301/301 [00:00<00:00, 21367.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting {map_author(path)}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 301/301 [00:00<00:00, 20218.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting {map_author(path)}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 301/301 [00:00<00:00, 16581.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting {map_author(path)}:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Write to path - keep data in line with sklearn load data\n",
    "w_path = 'cleanData/'\n",
    "\n",
    "# n defines words to have in each doc\n",
    "n = 500\n",
    "\n",
    "# Get the corpus for each author and split them in 500 word files\n",
    "# save them in the cleanData folder.\n",
    "for path in paths:\n",
    "    print(f'Segmenting {map_author(path)}:')\n",
    "    corpus = None\n",
    "    with open(path + 'corpus.txt') as file:\n",
    "        corpus =  file.read()\n",
    "    #corpus = open(f\"{path + 'corpus.txt'}\", \"r\").read()\n",
    "    corpus = clean_doc(corpus)\n",
    "    corpus = re.split(r'(\\W+)', corpus)\n",
    "    if len(corpus) // n < 300:\n",
    "        splits = len(corpus) // n\n",
    "        if len(corpus) % n > 0: splits += 1\n",
    "    else:\n",
    "        splits = 300\n",
    "    if len(corpus) % n > 0: splits += 1\n",
    "    cut = n\n",
    "    filename = (\"/corpus_part_%03i.txt\" % i for i in count(1))\n",
    "    \n",
    "    \n",
    "    for i in range(splits):\n",
    "        seg = ' '.join(corpus[(n*i):cut])\n",
    "        with open(w_path + map_author(path) + next(filename), \"w\") as file:\n",
    "            file.write(seg)\n",
    "        cut += n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "With our cleaned and balanced data, we load the data into sklearn with the load_files function. We split our data and fit the NB model with an sklearn pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sklearn load_data it will deduce the target variables\n",
    "# from the folder names - in our case the authors\n",
    "book_data = load_files('cleanData/', encoding=\"UTF-8\")\n",
    "\n",
    "# Split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(book_data.data, book_data.target) #random_state=93\n",
    "\n",
    "NB_clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1,1))),\n",
    "    ('clf', MultinomialNB(alpha=0.01)),\n",
    "])\n",
    "\n",
    "NB_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate our score, we use sklearn crossvalidation on the training data. Thereafter, we use the model to predict the values of our aforemention excluded test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 0.98148148, 1.        , 0.98148148,\n",
       "       1.        , 0.98148148, 0.99074074, 0.99074074, 1.        ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(NB_clf, X_train, y_train, cv=10)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9944444444444445"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = NB_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testData/1000words/text6_clean.txt joseRodriguesSantos\n",
      "testData/1000words/text5_clean.txt camiloCasteloBranco\n",
      "testData/1000words/text2_clean.txt almadaNegreiros\n",
      "testData/1000words/text3_clean.txt luisaMarquesSilva\n",
      "testData/1000words/text1_clean.txt joseSaramago\n",
      "testData/1000words/text4_clean.txt ecaDeQueiros\n"
     ]
    }
   ],
   "source": [
    "words500 = glob.glob('testData/1000words/*clean.txt')\n",
    "\n",
    "docs_new = [clean_new_data(open(path, 'r').read()) for path in words500]\n",
    "\n",
    "for i in range(len(words500)):\n",
    "    auth = int(NB_clf.predict([docs_new[i]]))\n",
    "    print(words500[i], book_data.target_names[auth])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    almadaNegreiros       1.00      0.96      0.98        25\n",
      "camiloCasteloBranco       1.00      1.00      1.00        78\n",
      "       ecaDeQueiros       1.00      1.00      1.00        75\n",
      "joseRodriguesSantos       1.00      1.00      1.00        77\n",
      "       joseSaramago       0.97      1.00      0.99        69\n",
      "  luisaMarquesSilva       1.00      0.97      0.99        36\n",
      "\n",
      "           accuracy                           0.99       360\n",
      "          macro avg       1.00      0.99      0.99       360\n",
      "       weighted avg       0.99      0.99      0.99       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted, target_names=book_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[24,  0,  0,  0,  1,  0],\n",
       "       [ 0, 78,  0,  0,  0,  0],\n",
       "       [ 0,  0, 75,  0,  0,  0],\n",
       "       [ 0,  0,  0, 77,  0,  0],\n",
       "       [ 0,  0,  0,  0, 69,  0],\n",
       "       [ 0,  0,  0,  0,  1, 35]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9472222222222222"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1,2))),\n",
    "    ('clf', SGDClassifier(alpha=0.01)),\n",
    "])\n",
    "\n",
    "SVM_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = SVM_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    almadaNegreiros       1.00      0.80      0.89        25\n",
      "camiloCasteloBranco       0.99      1.00      0.99        78\n",
      "       ecaDeQueiros       0.99      1.00      0.99        75\n",
      "joseRodriguesSantos       0.97      0.97      0.97        77\n",
      "       joseSaramago       0.82      1.00      0.90        69\n",
      "  luisaMarquesSilva       1.00      0.67      0.80        36\n",
      "\n",
      "           accuracy                           0.95       360\n",
      "          macro avg       0.96      0.91      0.93       360\n",
      "       weighted avg       0.95      0.95      0.95       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted, target_names=book_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20,  1,  1,  0,  3,  0],\n",
       "       [ 0, 78,  0,  0,  0,  0],\n",
       "       [ 0,  0, 75,  0,  0,  0],\n",
       "       [ 0,  0,  0, 75,  2,  0],\n",
       "       [ 0,  0,  0,  0, 69,  0],\n",
       "       [ 0,  0,  0,  2, 10, 24]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testData/500words/text6_clean.txt joseSaramago\n",
      "testData/500words/text5_clean.txt camiloCasteloBranco\n",
      "testData/500words/text2_clean.txt almadaNegreiros\n",
      "testData/500words/text3_clean.txt joseSaramago\n",
      "testData/500words/text1_clean.txt joseSaramago\n",
      "testData/500words/text4_clean.txt ecaDeQueiros\n"
     ]
    }
   ],
   "source": [
    "words500 = glob.glob('testData/500words/*clean.txt')\n",
    "\n",
    "docs_new = [clean_new_data(open(path, 'r').read()) for path in words500]\n",
    "\n",
    "for i in range(len(words500)):\n",
    "    auth = int(SVM_clf.predict([docs_new[i]]))\n",
    "    print(words500[i], book_data.target_names[auth])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit150444f68f124ddb8c8a027ea9e0ddd3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
