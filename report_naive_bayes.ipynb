{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Author Attribution\n",
    "\n",
    "## Identifying authors through excerpts\n",
    "\n",
    "------\n",
    "\n",
    "A notebook using Python alongside sklearn and NLTK to develop an NLP classifier. Given a collection of data on six Portuguese authors, we first analyze the data and take some general decisions on how to tackle the task. We make use of NLTK features to clean the data alongside some BASH scripts. The models are built and trained with sklearn.\n",
    "\n",
    "Final predictions based on best model:\n",
    "\n",
    "1000 word excerpts:\n",
    "\n",
    "- testData/1000words/text1_clean.txt: joseSaramago\n",
    "- testData/1000words/text2_clean.txt: almadaNegreiros\n",
    "- testData/1000words/text3_clean.txt: luisaMarquesSilva\n",
    "- testData/1000words/text4_clean.txt: ecaDeQueiros\n",
    "- testData/1000words/text5_clean.txt: camiloCasteloBranco\n",
    "- testData/1000words/text6_clean.txt: joseRodriguesSantos\n",
    "\n",
    "500 word excerpts:\n",
    "\n",
    "- testData/500words/text1_clean.txt: joseSaramago\n",
    "- testData/500words/text2_clean.txt: almadaNegreiros\n",
    "- testData/500words/text3_clean.txt: luisaMarquesSilva\n",
    "- testData/500words/text4_clean.txt: ecaDeQueiros\n",
    "- testData/500words/text5_clean.txt: camiloCasteloBranco\n",
    "- testData/5000words/text6_clean.txt: joseRodriguesSantos\n",
    "\n",
    "------\n",
    "Authors: \n",
    "- Davide Montali M20190201\n",
    "- Francisco Cruz M20190637\n",
    "- Umberto Tammaro M20190806\n",
    "\n",
    "Course: Text Mining -- Nova IMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "# Requirements and Imports\n",
    "\n",
    "Please ensure you have the dependencies below installed and make sure to download the required NLTK data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from itertools import count\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/d4ve/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please make sure you have the following parts downloaded\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalising\n",
    "\n",
    "We use a quick bash script to normalize the data. We first of all translate the data into ascii standard, to remove the various portuguese accents. Then we take the complement of the following set: 'A-Za-z0-9-,.!?\"' - we decided to keep the punctuation given the varying use of punctuation by the different authors. We also lowercase the documents and merge them into a corpus document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "unzip -q project-20200323.zip\n",
    "rm -rf trainData testData cleanData\n",
    "mkdir trainData testData cleanData\n",
    "mv project/Corpora/train/* trainData/\n",
    "mv project/Corpora/test/* testData/\n",
    "rm -rf project/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf trainData/*/*clean.txt\n",
    "rm -rf trainData/*/corpus.txt\n",
    "for dir in trainData/*/; do\n",
    "    for book in $dir*.txt; do\n",
    "        iconv -f utf8 -t ascii//TRANSLIT \"$book\" | tr -sc 'A-Za-z0-9-,.!?;:' ' ' | tr A-Z a-z  >> \"${dir}corpus.txt\"\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also apply the same script to the data to be identified, although here for obvious reasons we don't concatenate the files into a corpus document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf testData/*/*clean.txt\n",
    "for dir in testData/*/; do\n",
    "    for book in $dir*.txt; do\n",
    "        iconv -f utf8 -t ascii//TRANSLIT \"$book\" | tr -sc 'A-Za-z0-9-,.!?\"' ' ' | tr A-Z a-z  > \"${book%.*}\"_clean.txt\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our functions used throughout the notebook as well as some variables we use. We also load the NLTK portuguese stemmer and stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_author(path):\n",
    "    \"\"\"Determine the author of a book through its file path.\"\"\"\n",
    "    for key, author in authors.items():\n",
    "        if author.lower() in path.lower():\n",
    "            return author\n",
    "        \n",
    "def clean_doc(doc, stopwords=True):\n",
    "    doc = stem_doc(doc)\n",
    "    if stopwords == True:\n",
    "        doc = stop_doc(doc)\n",
    "    return doc\n",
    "\n",
    "def stem_doc(doc):\n",
    "    \"\"\"Takes a document, splits it up and stemms each word - then\n",
    "    remerges the document together and returns it.\"\"\"\n",
    "    doc_split = doc.split()\n",
    "    stem = [stemmer.stem(str(i)) for i in doc_split]\n",
    "    doc = ' '.join(stem)\n",
    "    return doc\n",
    "\n",
    "def stop_doc(doc):\n",
    "    \"\"\"Takes a document and removes all stopwords from it\"\"\"\n",
    "    doc_split = doc.split()\n",
    "    temp = [i for i in doc_split if i not in stopwords]\n",
    "    doc = ' '.join(temp)\n",
    "    return doc\n",
    "\n",
    "def clean_new_data(doc):\n",
    "    doc = re.split(r'(\\W+)', doc)\n",
    "    doc = ' '.join(doc)\n",
    "    doc = clean_doc(doc)\n",
    "    return doc\n",
    "\n",
    "unknown = glob.glob('testData/*/*clean.txt')\n",
    "unknown.sort()\n",
    "\n",
    "def predict_unknown(model):\n",
    "    for doc in unknown:\n",
    "        with open(doc) as file:\n",
    "            y = int(model.predict(file))\n",
    "            print(f\"{doc} --> {book_data.target_names[y]}\")\n",
    "\n",
    "authors = {\n",
    "            1: \"almadaNegreiros\",\n",
    "            2: \"ecaDeQueiros\",\n",
    "            3: \"joseSaramago\",\n",
    "            4: \"camiloCasteloBranco\",\n",
    "            5: \"joseRodriguesSantos\",\n",
    "            6: \"luisaMarquesSilva\"}\n",
    "\n",
    "# NLTK Tools\n",
    "stopwords = list(nltk.corpus.stopwords.words('portuguese'))\n",
    "stemmer = nltk.stem.RSLPStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing\n",
    "\n",
    "Given the large imbalance in terms of words per author we have, we decided to split the corpus of each author into a number of smaller documents. Below we split into 500 word documents - the same count as the shortest excerpts we are looking to predict.\n",
    "\n",
    "We also decided to undersample our data - this is further motivated by the vast imbalance ratio between the different auhtors. We explored oversampling as well - but given that we would generate new data for our minority class on the already scarce data we have, we decided undersampling was a more sensible route.\n",
    "\n",
    "We store our data in a separate folder, with documents within a subfolder named after the authors (our target variable) - using this format we can easily read our data into sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting almadaNegreiros:\n",
      "Done.\n",
      "\n",
      "Segmenting camiloCasteloBranco:\n",
      "Done.\n",
      "\n",
      "Segmenting joseRodriguesSantos:\n",
      "Done.\n",
      "\n",
      "Segmenting luisaMarquesSilva:\n",
      "Done.\n",
      "\n",
      "Segmenting joseSaramago:\n",
      "Done.\n",
      "\n",
      "Segmenting ecaDeQueiros:\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training data paths\n",
    "corpora = glob.glob('trainData/*/corpus.txt')\n",
    "\n",
    "# Create a function to split the data into smaller files, and save them to file\n",
    "# use the sklearn dataset format - so later files can be worked with easily.\n",
    "\n",
    "def trucate(paths, w_path='cleanData/', words=500, max_splits=300):\n",
    "    \"\"\"Truncate function takes a list of corpora paths, and splits them\n",
    "    based on the parameters set. The default is to write the files to a new\n",
    "    folder with the authors as subdirectories and splitting documents into 500\n",
    "    word docs, with a maximum of 300 splits per author. -1 splits makes all splits\n",
    "    possible, without undersampling\"\"\" \n",
    "    \n",
    "    # Make sure previous splits are deleted, to avoid errors\n",
    "    # regarding split numbers or max words\n",
    "    shutil.rmtree(w_path)\n",
    "    \n",
    "    for path in paths:\n",
    "        print(f'Segmenting {map_author(path)}:')\n",
    "        \n",
    "        corpus = None\n",
    "        \n",
    "        with open(path) as file:\n",
    "            corpus = file.read()\n",
    "        \n",
    "        corpus = clean_doc(corpus)\n",
    "        corpus = re.split(r'(\\W+)', corpus)\n",
    "        print('Done.\\n')\n",
    "        \n",
    "        if max_splits == -1:\n",
    "            splits = len(corpus) // words\n",
    "            # Make sure we don't loose data due to floor division.\n",
    "            if len(corpus) % words > 0: splits += 1\n",
    "        elif len(corpus) // words < max_splits:\n",
    "            splits = len(corpus) // words\n",
    "            # Make sure we don't loose data due to floor division.\n",
    "            if len(corpus) % words > 0: splits += 1\n",
    "        else:\n",
    "            splits = max_splits\n",
    "        if len(corpus) % words > 0: splits += 1\n",
    "        \n",
    "        # Set filename with itertools\n",
    "        filename = (\"/corpus_part_%03i.txt\" % i for i in count(1))\n",
    "        \n",
    "        # Check if directories exist, and except errors\n",
    "        # in case they do, otherwise create them.\n",
    "        path_auth = w_path + map_author(path)\n",
    "        Path(path_auth).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        cut = words\n",
    "        for i in range(splits - 1):\n",
    "            seg = ' '.join(corpus[(words*i):cut])\n",
    "            seg = re.sub(' +', ' ', seg)\n",
    "            with open(path_auth + next(filename), \"w\") as file:\n",
    "                file.write(seg)\n",
    "            cut += words\n",
    "            \n",
    "trucate(corpora, words=1000, max_splits=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "With our cleaned and balanced data, we load the data into sklearn with the load_files function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn load_data it will deduce the target variables\n",
    "# from the folder names - in our case the authors\n",
    "book_data = load_files('cleanData/', encoding=\"UTF-8\")\n",
    "\n",
    "# Split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(book_data.data, book_data.target, train_size = 0.75) #random_state=93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "We split our data and fit the NB model with an sklearn pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b|!|\\\\?|\\\\.+|,|-+|:|;',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use sklearn pipeline to create our NB classifier\n",
    "NB_clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1,1), token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\.+|,|-+|:|;\")),\n",
    "    ('clf', MultinomialNB(alpha=0.01)), # We add smoothing to our NB \n",
    "])\n",
    "\n",
    "NB_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To validate our score, we use sklearn crossvalidation on the training data. Thereafter, we use the model to predict the values of our aforemention excluded test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98113208, 0.96855346, 0.97484277, 0.97484277, 0.98742138,\n",
       "       0.97484277, 0.97484277, 0.98113208, 0.97468354, 0.97468354])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(NB_clf, X_train, y_train, cv=10)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.969811320754717"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = NB_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testData/1000Palavras/text1_clean.txt --> joseSaramago\n",
      "testData/1000Palavras/text2_clean.txt --> joseSaramago\n",
      "testData/1000Palavras/text3_clean.txt --> joseSaramago\n",
      "testData/1000Palavras/text4_clean.txt --> ecaDeQueiros\n",
      "testData/1000Palavras/text5_clean.txt --> camiloCasteloBranco\n",
      "testData/1000Palavras/text6_clean.txt --> joseRodriguesSantos\n",
      "testData/500Palavras/text1_clean.txt --> joseSaramago\n",
      "testData/500Palavras/text2_clean.txt --> joseSaramago\n",
      "testData/500Palavras/text3_clean.txt --> joseSaramago\n",
      "testData/500Palavras/text4_clean.txt --> ecaDeQueiros\n",
      "testData/500Palavras/text5_clean.txt --> camiloCasteloBranco\n",
      "testData/500Palavras/text6_clean.txt --> joseSaramago\n"
     ]
    }
   ],
   "source": [
    "predict_unknown(NB_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    almadaNegreiros       1.00      0.53      0.69        17\n",
      "camiloCasteloBranco       1.00      1.00      1.00       132\n",
      "       ecaDeQueiros       0.95      1.00      0.97       128\n",
      "joseRodriguesSantos       0.98      1.00      0.99       113\n",
      "       joseSaramago       0.95      1.00      0.97       123\n",
      "  luisaMarquesSilva       1.00      0.53      0.69        17\n",
      "\n",
      "           accuracy                           0.97       530\n",
      "          macro avg       0.98      0.84      0.89       530\n",
      "       weighted avg       0.97      0.97      0.97       530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted, target_names=book_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9,   0,   7,   0,   1,   0],\n",
       "       [  0, 132,   0,   0,   0,   0],\n",
       "       [  0,   0, 128,   0,   0,   0],\n",
       "       [  0,   0,   0, 113,   0,   0],\n",
       "       [  0,   0,   0,   0, 123,   0],\n",
       "       [  0,   0,   0,   2,   6,   9]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9905660377358491"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "    ('clf', SGDClassifier(alpha=0.01)),\n",
    "])\n",
    "\n",
    "SVM_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = SVM_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    almadaNegreiros       1.00      0.88      0.94        17\n",
      "camiloCasteloBranco       1.00      0.99      1.00       132\n",
      "       ecaDeQueiros       0.99      1.00      1.00       128\n",
      "joseRodriguesSantos       1.00      1.00      1.00       113\n",
      "       joseSaramago       0.97      1.00      0.98       123\n",
      "  luisaMarquesSilva       1.00      0.88      0.94        17\n",
      "\n",
      "           accuracy                           0.99       530\n",
      "          macro avg       0.99      0.96      0.98       530\n",
      "       weighted avg       0.99      0.99      0.99       530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted, target_names=book_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32,  1,  0,  0,  0,  0],\n",
       "       [ 0, 66,  0,  0,  0,  0],\n",
       "       [ 0,  0, 81,  0,  0,  0],\n",
       "       [ 0,  0,  0, 79,  0,  0],\n",
       "       [ 0,  0,  0,  0, 73,  0],\n",
       "       [ 0,  0,  0,  0,  0, 27]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testData/1000Palavras/text1_clean.txt --> joseSaramago\n",
      "testData/1000Palavras/text2_clean.txt --> joseRodriguesSantos\n",
      "testData/1000Palavras/text3_clean.txt --> joseRodriguesSantos\n",
      "testData/1000Palavras/text4_clean.txt --> joseRodriguesSantos\n",
      "testData/1000Palavras/text5_clean.txt --> joseRodriguesSantos\n",
      "testData/1000Palavras/text6_clean.txt --> joseRodriguesSantos\n",
      "testData/500Palavras/text1_clean.txt --> joseSaramago\n",
      "testData/500Palavras/text2_clean.txt --> joseSaramago\n",
      "testData/500Palavras/text3_clean.txt --> joseRodriguesSantos\n",
      "testData/500Palavras/text4_clean.txt --> ecaDeQueiros\n",
      "testData/500Palavras/text5_clean.txt --> joseRodriguesSantos\n",
      "testData/500Palavras/text6_clean.txt --> joseRodriguesSantos\n"
     ]
    }
   ],
   "source": [
    "predict_unknown(SVM_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9679245283018868"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_clf = Pipeline([\n",
    "    ('vect', TfidfVectorizer(ngram_range=(1,1))),\n",
    "    ('clf', KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='brute', leaf_size=30, p=2,\n",
    "                                metric='cosine', metric_params=None, n_jobs=1)),\n",
    "])\n",
    "\n",
    "\n",
    "KNN_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = KNN_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testData/1000Palavras/text1_clean.txt --> joseSaramago\n",
      "testData/1000Palavras/text2_clean.txt --> ecaDeQueiros\n",
      "testData/1000Palavras/text3_clean.txt --> joseRodriguesSantos\n",
      "testData/1000Palavras/text4_clean.txt --> ecaDeQueiros\n",
      "testData/1000Palavras/text5_clean.txt --> ecaDeQueiros\n",
      "testData/1000Palavras/text6_clean.txt --> joseRodriguesSantos\n",
      "testData/500Palavras/text1_clean.txt --> joseSaramago\n",
      "testData/500Palavras/text2_clean.txt --> ecaDeQueiros\n",
      "testData/500Palavras/text3_clean.txt --> joseRodriguesSantos\n",
      "testData/500Palavras/text4_clean.txt --> ecaDeQueiros\n",
      "testData/500Palavras/text5_clean.txt --> ecaDeQueiros\n",
      "testData/500Palavras/text6_clean.txt --> joseRodriguesSantos\n"
     ]
    }
   ],
   "source": [
    "predict_unknown(KNN_clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit150444f68f124ddb8c8a027ea9e0ddd3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
