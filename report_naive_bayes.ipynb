{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP - Author Attribution\n",
    "\n",
    "- Data Prep\n",
    "- Corpora\n",
    "- Crossvalidation\n",
    "- Baseline\n",
    "- Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import glob\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import CategoricalNB, MultinomialNB\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/d4ve/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please make sure you have the following parts downloaded\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a quick bash loop to create csv files with bag of words for all training books.\n",
    "Also clean all txt files by removing portuguse accents, removing all spacing and lower the casing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenising: trainData/almadaNegreiros/pg22615.txt\n",
      "Cleaning up: trainData/almadaNegreiros/pg22615.txt \n",
      "\n",
      "Tokenising: trainData/almadaNegreiros/pg22730.txt\n",
      "Cleaning up: trainData/almadaNegreiros/pg22730.txt \n",
      "\n",
      "Tokenising: trainData/almadaNegreiros/pg22801.txt\n",
      "Cleaning up: trainData/almadaNegreiros/pg22801.txt \n",
      "\n",
      "Tokenising: trainData/almadaNegreiros/pg22802.txt\n",
      "Cleaning up: trainData/almadaNegreiros/pg22802.txt \n",
      "\n",
      "Tokenising: trainData/almadaNegreiros/pg22969.txt\n",
      "Cleaning up: trainData/almadaNegreiros/pg22969.txt \n",
      "\n",
      "Tokenising: trainData/almadaNegreiros/pg23133.txt\n",
      "Cleaning up: trainData/almadaNegreiros/pg23133.txt \n",
      "\n",
      "Tokenising: trainData/almadaNegreiros/pg23620.txt\n",
      "Cleaning up: trainData/almadaNegreiros/pg23620.txt \n",
      "\n",
      "Tokenising: trainData/almadaNegreiros/pg23879.txt\n",
      "Cleaning up: trainData/almadaNegreiros/pg23879.txt \n",
      "\n",
      "Tokenising: trainData/almadaNegreiros/pg23961.txt\n",
      "Cleaning up: trainData/almadaNegreiros/pg23961.txt \n",
      "\n",
      "Tokenising: trainData/almadaNegreiros/tesst.txt\n",
      "Cleaning up: trainData/almadaNegreiros/tesst.txt \n",
      "\n",
      "Tokenising: trainData/almadaNegreiros/test.txt\n",
      "Cleaning up: trainData/almadaNegreiros/test.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/24691-0.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/24691-0.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/34756-0.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/34756-0.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg16425.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg16425.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg17927.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg17927.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg19375.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg19375.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg21406.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg21406.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg23203.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg23203.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg23345.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg23345.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg23346.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg23346.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg24339.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg24339.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg25844.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg25844.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg26017.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg26017.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg26103.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg26103.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg26110.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg26110.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg26988.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg26988.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg27364.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg27364.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg27541.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg27541.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg28310.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg28310.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg31694.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg31694.txt \n",
      "\n",
      "Tokenising: trainData/camiloCasteloBranco/pg33788.txt\n",
      "Cleaning up: trainData/camiloCasteloBranco/pg33788.txt \n",
      "\n",
      "Tokenising: trainData/ecaDeQueiros/pg18220.txt\n",
      "Cleaning up: trainData/ecaDeQueiros/pg18220.txt \n",
      "\n",
      "Tokenising: trainData/ecaDeQueiros/pg25641.txt\n",
      "Cleaning up: trainData/ecaDeQueiros/pg25641.txt \n",
      "\n",
      "Tokenising: trainData/ecaDeQueiros/pg27637.txt\n",
      "Cleaning up: trainData/ecaDeQueiros/pg27637.txt \n",
      "\n",
      "Tokenising: trainData/ecaDeQueiros/pg31347.txt\n",
      "Cleaning up: trainData/ecaDeQueiros/pg31347.txt \n",
      "\n",
      "Tokenising: trainData/ecaDeQueiros/pg40409.txt\n",
      "Cleaning up: trainData/ecaDeQueiros/pg40409.txt \n",
      "\n",
      "Tokenising: trainData/joseRodriguesSantos/A Filha Do Capitao - Jose Rodrigues dos Santos.txt\n",
      "Cleaning up: trainData/joseRodriguesSantos/A Filha Do Capitao - Jose Rodrigues dos Santos.txt \n",
      "\n",
      "Tokenising: trainData/joseRodriguesSantos/A Formula De Deus - Jose Rodrigues dos Santos.txt\n",
      "Cleaning up: trainData/joseRodriguesSantos/A Formula De Deus - Jose Rodrigues dos Santos.txt \n",
      "\n",
      "Tokenising: trainData/joseRodriguesSantos/A Mao do Diabo - Jose Rodrigues dos Santos.txt\n",
      "Cleaning up: trainData/joseRodriguesSantos/A Mao do Diabo - Jose Rodrigues dos Santos.txt \n",
      "\n",
      "Tokenising: trainData/joseRodriguesSantos/A Vida Num Sopro - Jose Rodrigues dos Santos.txt\n",
      "Cleaning up: trainData/joseRodriguesSantos/A Vida Num Sopro - Jose Rodrigues dos Santos.txt \n",
      "\n",
      "Tokenising: trainData/joseRodriguesSantos/Furia Divina - Jose Rodrigues dos Santos.txt\n",
      "Cleaning up: trainData/joseRodriguesSantos/Furia Divina - Jose Rodrigues dos Santos.txt \n",
      "\n",
      "Tokenising: trainData/joseRodriguesSantos/O Anjo Branco - Jose Rodrigues dos Santos.txt\n",
      "Cleaning up: trainData/joseRodriguesSantos/O Anjo Branco - Jose Rodrigues dos Santos.txt \n",
      "\n",
      "Tokenising: trainData/joseRodriguesSantos/O Setimo Selo - Jose Rodrigues dos Santos.txt\n",
      "Cleaning up: trainData/joseRodriguesSantos/O Setimo Selo - Jose Rodrigues dos Santos.txt \n",
      "\n",
      "Tokenising: trainData/joseRodriguesSantos/O ultimo Segredo - Jose Rodrigues dos Santos.txt\n",
      "Cleaning up: trainData/joseRodriguesSantos/O ultimo Segredo - Jose Rodrigues dos Santos.txt \n",
      "\n",
      "Tokenising: trainData/joseSaramago/A Caverna - Jose Saramago.txt\n",
      "Cleaning up: trainData/joseSaramago/A Caverna - Jose Saramago.txt \n",
      "\n",
      "Tokenising: trainData/joseSaramago/As Intermitencias da Morte - Jose Saramago.txt\n",
      "Cleaning up: trainData/joseSaramago/As Intermitencias da Morte - Jose Saramago.txt \n",
      "\n",
      "Tokenising: trainData/joseSaramago/Caim - Jose Saramago.txt\n",
      "Cleaning up: trainData/joseSaramago/Caim - Jose Saramago.txt \n",
      "\n",
      "Tokenising: trainData/joseSaramago/Claraboia - Jose Saramago.txt\n",
      "Cleaning up: trainData/joseSaramago/Claraboia - Jose Saramago.txt \n",
      "\n",
      "Tokenising: trainData/joseSaramago/Ensaio Sobre a Cegueira - Jose Saramago.txt\n",
      "Cleaning up: trainData/joseSaramago/Ensaio Sobre a Cegueira - Jose Saramago.txt \n",
      "\n",
      "Tokenising: trainData/joseSaramago/Historia Do Cerco De Lisboa - Jose Saramago.txt\n",
      "Cleaning up: trainData/joseSaramago/Historia Do Cerco De Lisboa - Jose Saramago.txt \n",
      "\n",
      "Tokenising: trainData/joseSaramago/Memorial Do Convento - Jose Saramago.txt\n",
      "Cleaning up: trainData/joseSaramago/Memorial Do Convento - Jose Saramago.txt \n",
      "\n",
      "Tokenising: trainData/joseSaramago/O Ano Da Morte De Ricardo Reis - Jose Saramago.txt\n",
      "Cleaning up: trainData/joseSaramago/O Ano Da Morte De Ricardo Reis - Jose Saramago.txt \n",
      "\n",
      "Tokenising: trainData/joseSaramago/O Conto Da Ilha Desconhecida - Jose Saramago.txt\n",
      "Cleaning up: trainData/joseSaramago/O Conto Da Ilha Desconhecida - Jose Saramago.txt \n",
      "\n",
      "Tokenising: trainData/joseSaramago/O Homem Duplicado - Jose Saramago.txt\n",
      "Cleaning up: trainData/joseSaramago/O Homem Duplicado - Jose Saramago.txt \n",
      "\n",
      "Tokenising: trainData/joseSaramago/Terra Do Pecado - Jose Saramago.txt\n",
      "Cleaning up: trainData/joseSaramago/Terra Do Pecado - Jose Saramago.txt \n",
      "\n",
      "Tokenising: trainData/joseSaramago/Viagem Do Elefante - Jose Saramago.txt\n",
      "Cleaning up: trainData/joseSaramago/Viagem Do Elefante - Jose Saramago.txt \n",
      "\n",
      "Tokenising: trainData/luisaMarquesSilva/ABelaHistoria.txt\n",
      "Cleaning up: trainData/luisaMarquesSilva/ABelaHistoria.txt \n",
      "\n",
      "Tokenising: trainData/luisaMarquesSilva/acabouSe.txt\n",
      "Cleaning up: trainData/luisaMarquesSilva/acabouSe.txt \n",
      "\n",
      "Tokenising: trainData/luisaMarquesSilva/Botão.txt\n",
      "Cleaning up: trainData/luisaMarquesSilva/Botão.txt \n",
      "\n",
      "Tokenising: trainData/luisaMarquesSilva/controlz.txt\n",
      "Cleaning up: trainData/luisaMarquesSilva/controlz.txt \n",
      "\n",
      "Tokenising: trainData/luisaMarquesSilva/emedo.txt\n",
      "Cleaning up: trainData/luisaMarquesSilva/emedo.txt \n",
      "\n",
      "Tokenising: trainData/luisaMarquesSilva/Lisboa2050.txt\n",
      "Cleaning up: trainData/luisaMarquesSilva/Lisboa2050.txt \n",
      "\n",
      "Tokenising: trainData/luisaMarquesSilva/passeioInferno.txt\n",
      "Cleaning up: trainData/luisaMarquesSilva/passeioInferno.txt \n",
      "\n",
      "Tokenising: trainData/luisaMarquesSilva/rapsodiasemdo.txt\n",
      "Cleaning up: trainData/luisaMarquesSilva/rapsodiasemdo.txt \n",
      "\n",
      "Tokenising: trainData/luisaMarquesSilva/UltimaHistoria.txt\n",
      "Cleaning up: trainData/luisaMarquesSilva/UltimaHistoria.txt \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf trainData/*/*tokenised.csv\n",
    "rm -rf trainData/*/*clean.txt\n",
    "for dir in trainData/*/; do\n",
    "    for book in $dir*.txt; do\n",
    "        printf \"Tokenising: $book\\n\"\n",
    "        tr -sc 'A-Za-z' '\\n' < \"$book\" | tr A-Z a-z | sort | uniq -c | sort -nr | awk '{print $1, $2}' | tr \" \" \",\" > \"${book%.*}\"_tokenised.csv\n",
    "        printf \"Cleaning up: $book \\n\\n\"\n",
    "        iconv -f utf8 -t ascii//TRANSLIT \"$book\" | tr -sc 'A-Za-z0-9' ' ' | tr A-Z a-z  > \"${book%.*}\"_clean.txt\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = {\n",
    "            1: \"almadaNegreiros\",\n",
    "            2: \"ecaDeQueiros\",\n",
    "            3: \"joseSaramago\",\n",
    "            4: \"camiloCasteloBranco\",\n",
    "            5: \"joseRodriguesSantos\",\n",
    "            6: \"luisaMarquesSilva\"}\n",
    "\n",
    "# NLTK Tools\n",
    "stopwords = list(nltk.corpus.stopwords.words('portuguese'))\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "# Training data paths\n",
    "paths = glob.glob('trainData/*/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_author(path):\n",
    "    \"\"\"Determine the author of a book through its ffile path.\"\"\"\n",
    "    for key, author in authors.items():\n",
    "        if author in path:\n",
    "            return author\n",
    "        \n",
    "def clean_doc(doc, stopwords=True):\n",
    "    doc = stem_doc(doc)\n",
    "    if stopwords == True:\n",
    "        doc = stop_doc(doc)\n",
    "    return doc\n",
    "\n",
    "def stem_doc(doc):\n",
    "    \"\"\"Takes a document, splits it up and stemms each word - then\n",
    "    remerges the document together and returns it.\"\"\"\n",
    "    doc_split = doc.split()\n",
    "    stem = [stemmer.stem(str(i)) for i in doc_split]\n",
    "    doc = ' '.join(stem)\n",
    "    return doc\n",
    "\n",
    "def stop_doc(doc):\n",
    "    \"\"\"Takes a document and removes all stopwords from it\"\"\"\n",
    "    doc_split = doc.split()\n",
    "    temp = [i for i in doc_split if i not in stopwords]\n",
    "    doc = ' '.join(temp)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "# Write to path - keep data in line with sklearn load data\n",
    "w_path = 'cleanData/'\n",
    "\n",
    "# n defines words to have in each doc\n",
    "n = 500\n",
    "\n",
    "# Get the corpus for each author and split them in 500 word files\n",
    "# save them in the cleanData folder.\n",
    "for path in paths:\n",
    "    corpus = open(f\"{path + 'corpus.txt'}\", \"r\").read()\n",
    "    corpus = clean_doc(corpus)\n",
    "    corpus.split()\n",
    "    splits = len(corpus) // n\n",
    "    if len(corpus) % n > 0: splits += 1\n",
    "    cut = n\n",
    "    filename = (\"/corpus_part_%03i.txt\" % i for i in count(1))\n",
    "    for i in range(splits):\n",
    "        seg = ''.join(corpus[(n*i):cut])\n",
    "        with open(w_path + map_author(path) + next(filename), \"w\") as file:\n",
    "            file.write(seg)\n",
    "        cut += n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sklearn load_data it will deduce the target variables\n",
    "# from the folder names - in our case the authors\n",
    "book_data = load_files('cleanData/', encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(book_data.data, book_data.target, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB(alpha=1.0e-10)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1e-10, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9416806722689076"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    almadaNegreiros       1.00      0.21      0.34        73\n",
      "camiloCasteloBranco       0.94      0.99      0.97      1310\n",
      "       ecaDeQueiros       0.96      0.92      0.94       794\n",
      "joseRodriguesSantos       0.93      0.97      0.95      2013\n",
      "       joseSaramago       0.94      0.94      0.94      1686\n",
      "  luisaMarquesSilva       1.00      0.18      0.30        74\n",
      "\n",
      "           accuracy                           0.94      5950\n",
      "          macro avg       0.96      0.70      0.74      5950\n",
      "       weighted avg       0.94      0.94      0.94      5950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted, target_names=book_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  15,   20,   19,   10,    9,    0],\n",
       "       [   0, 1303,    6,    1,    0,    0],\n",
       "       [   0,   50,  731,    3,   10,    0],\n",
       "       [   0,    1,    0, 1961,   51,    0],\n",
       "       [   0,    5,    4,   97, 1580,    0],\n",
       "       [   0,    1,    0,   34,   26,   13]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9329411764705883"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 2))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                          alpha=0.01, random_state=42,\n",
    "                          max_iter=5, tol=None)),\n",
    "])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    almadaNegreiros       1.00      0.01      0.03        73\n",
      "camiloCasteloBranco       0.91      0.98      0.95      1310\n",
      "       ecaDeQueiros       0.99      0.80      0.89       794\n",
      "joseRodriguesSantos       0.95      0.98      0.96      2013\n",
      "       joseSaramago       0.91      0.97      0.94      1686\n",
      "  luisaMarquesSilva       1.00      0.04      0.08        74\n",
      "\n",
      "           accuracy                           0.93      5950\n",
      "          macro avg       0.96      0.63      0.64      5950\n",
      "       weighted avg       0.94      0.93      0.92      5950\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, predicted, target_names=book_data.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,   27,    4,   10,   31,    0],\n",
       "       [   0, 1289,    1,    7,   13,    0],\n",
       "       [   0,   94,  636,   21,   43,    0],\n",
       "       [   0,    3,    0, 1980,   30,    0],\n",
       "       [   0,    1,    0,   43, 1642,    0],\n",
       "       [   0,    2,    0,   33,   36,    3]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 0.01, 'tfidf__use_idf': True, 'vect__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit150444f68f124ddb8c8a027ea9e0ddd3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
